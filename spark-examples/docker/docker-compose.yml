version: '3'
services:
  spark-master:
    read_only: true
    build: .
    image: local/spark-2
    command: start-hadoop-namenode
    command: spark-class org.apache.spark.deploy.master.Master
    environment:
      MASTER: spark://spark-master:7077
      SPARK_PUBLIC_DNS: 127.0.0.1
    ports:
      - 4040:4040
      - 6066:6066
      - 8080:8080
      - 18080:18080
    volumes:
      - "$HDFS_VOL1"
      - "$HDFS_VOL2"
      - ./volume:/mnt/volume:ro

  secondarynamenode:
    read_only: true
    image: local/hadoop-2
    build: .
    links:
      - spark-master:namenode
    command: start-hadoop-secondarynamenode
    volumes:
      - "$HDFS_VOL1"

  spark-worker:
    read_only: true
    deploy:
      replicas: 2
    image: local/spark-2
    command: start-hadoop-datanode
    command: spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    links:
      - spark-master:namenode
      - secondarynamenode
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: 127.0.0.1
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
    volumes:
      - "$HDFS_VOL1"
