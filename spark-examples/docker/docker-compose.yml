version: '3'
services:
  fluentd:
    image: fluent/fluentd
    environment:
      FLUENTD_CONF: stdout.conf
    volumes:
      - ./volume/conf/fluentd:/fluentd/etc:ro

  spark-master:
    read_only: true
    build: .
    links:
      - fluentd
    image: local/spark-2
    #hostname: spark-master
    #command: sleep 24h
    command: start-spark-master
    environment:
      MASTER: spark://spark-master:7077
    ports:
      - 4040:4040
      - 6066:6066
      - 8080:8080
      - 18080:18080
    volumes:
      - "$HDFS_VOL1"
      - "$HDFS_VOL2"
      - "$USER_TMP"
      - ./volume/code:$USER_CODE_MNT:ro
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro
      - ./volume/conf/spark:$SPARK_CONF_DIR:ro

  secondarynamenode:
    read_only: true
    image: local/spark-2
    build: .
    links:
      - spark-master
      - fluentd
    #command: sleep 24h
    command: start-hadoop-secondarynamenode
    volumes:
      - "$HDFS_VOL1"
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro

  spark-worker:
    read_only: true
    deploy:
      replicas: 2
    image: local/spark-2
    #command: sleep 24h
    command: start-spark-worker
    links:
      - fluentd
      - spark-master
      - secondarynamenode
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
    volumes:
      - "$HDFS_VOL1"
      - "$USER_TMP"
      - ./volume/code:$USER_CODE_MNT:ro
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro
      - ./volume/conf/spark:$SPARK_CONF_DIR:ro
