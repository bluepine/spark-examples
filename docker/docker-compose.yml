version: '3'
services:
  fluentd:
    image: fluent/fluentd
    environment:
      FLUENTD_CONF: stdout.conf
    volumes:
      - ./volume/conf/fluentd:/fluentd/etc:ro

  spark-master:
    read_only: true
    build: .
    depends_on:
      - fluentd
    image: local/spark-2
    hostname: spark-master #tricky: this settings is necessary for docker swarm mode if the hostname "spark-master" is used in hadoop conf file for hdfs namenode. the reason is that the service name in docker swarm mode resolves to a forwarding address which is different from the address of any service instances. hence if a service instance needs to resolve said service name to itself we need to set hostname for it explicitly here.
    command: start-spark-master
    environment:
      MASTER: spark://spark-master:7077
    ports:
      - 8080:8080 #spark jobs web ui
      - 18080:18080 #spark history web ui
      - 50070:50070 #hdfs namenode web ui
    volumes:
      - ./volume/code:$USER_CODE_MNT:ro
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro
      - ./volume/conf/spark:$SPARK_CONF_DIR:ro
      - "$HDFS_VOL1"
      - "$HDFS_VOL2"
      - "$USER_TMP"

  secondarynamenode:
    read_only: true
    image: local/spark-2
    build: .
    depends_on:
      - spark-master
      - fluentd
    command: start-hadoop-secondarynamenode
    volumes:
      - ./volume/code:$USER_CODE_MNT:ro
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro
      - "$HDFS_VOL1"

  spark-worker:
    read_only: true
    deploy:
      replicas: 2
    image: local/spark-2
    command: start-spark-worker
    depends_on:
      - fluentd
      - spark-master
      - secondarynamenode
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
    volumes:
      - ./volume/code:$USER_CODE_MNT:ro
      - ./volume/conf/hadoop:$HADOOP_CONF_DIR:ro
      - ./volume/conf/spark:$SPARK_CONF_DIR:ro
      - "$HDFS_VOL1"
      - "$USER_TMP"
